{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72HQYCSKGSy2"
      },
      "source": [
        "# Access and query remote data \n",
        "\n",
        " 1. Execute SPARQL queries on a local RDF file\n",
        " 2. Query a remote SPARQL endpoint\n",
        " 3. Data integration\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8DwfhYTGSzF"
      },
      "source": [
        "## 1. Execute SPARQL queries on a local RDF file\n",
        "\n",
        "RDFlib allows us to iterate over triples, and also to perform SPARQL queries on local data (e.g. a file):\n",
        "\n",
        " * parse the RDF file into a RDFLib graph\n",
        " * prepare a SPARQL query as a string\n",
        " * use the RDFLib method `query` to query the graph\n",
        " * iterate over results of the query\n",
        " \n",
        "Results are organised in a **list of tuples**. Every tuple includes as many values as the number of query variables, served in the same order as they appear in the `SELECT` clause. \n",
        "Values of the tuples can be accessed by position (e.g. `query_res[0]`) or by variable name (e.g. `query_res[\"class\"]`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5xuqWQYGSzG",
        "outputId": "85ac0362-a89f-4251-c656-7c57eecf89d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n",
            "\u001b[K     |████████████████████████████████| 482 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 756 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-6.1.1\n",
            "http://www.wikidata.org/entity/Q9388534 25\n",
            "http://www.wikidata.org/entity/Q5 24\n",
            "http://www.wikidata.org/entity/Q31855 5\n"
          ]
        }
      ],
      "source": [
        "#uncomment if colab\n",
        "#!pip install rdflib \n",
        "import rdflib\n",
        "\n",
        "# create an empty Graph\n",
        "g = rdflib.ConjunctiveGraph()\n",
        "\n",
        "# parse a local RDF file by specifying the format\n",
        "result = g.parse(\"https://raw.githubusercontent.com/marilenadaquino/information_visualization/main/2022-2023/resources/artchives.nq\", format='nquads')\n",
        "\n",
        "# perform the query over the graph\n",
        "query_results = g.query(\n",
        "    \"\"\"SELECT ?class (COUNT(?individual) AS ?tot)\n",
        "       WHERE { ?individual a ?class .}\n",
        "       GROUP BY ?class ?tot\"\"\")\n",
        "\n",
        "# retrieve a list of tuples\n",
        "for query_res in query_results:\n",
        "    print(query_res[0], query_res[\"tot\"]) # notice the two alternative ways to recall values in the tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vJCqMzzqokE7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk8fzcUpGSzD"
      },
      "source": [
        "## 2. Query a remote SPARQL endpoint\n",
        "\n",
        "To query an endpoint you have several strategies:\n",
        "\n",
        " * **query the endpoint from a GUI**, download results (e.g. in JSON format) and use python to parse results (time-consuming, but better for big queries)\n",
        " * **query the endpoint REST API** with python, retrieve data in the desired format, parse it as you like (faster to repeat, but ineffective if you have big queries, due to potential limits of the endpoint)\n",
        "\n",
        "\n",
        "### 2.1 Query the endpoint from a GUI\n",
        "\n",
        "A SPARQL endpoint usually offers a number of operations that you can perform on results:\n",
        "\n",
        " * **display** data in several formats (tabular, json raw response, charts)\n",
        " * **download** data as JSON objects, CSV tables, and XML documents.\n",
        "\n",
        "**Note for future yourself. What format to choose for download?** It depends. JSON is usually better for manipulating results (deduplication, cleaning, enrichment). \n",
        "\n",
        "Let's perform the previous example query on the ARTchives endpoint, and have a look at the JSON result of the query (select the tab raw response)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQRNZghgGSzD"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT ?class (COUNT(?individual) AS ?tot)\n",
        "       WHERE { ?individual a ?class .}\n",
        "       GROUP BY ?class ?tot\"\"\"\n",
        "\n",
        "endpoint = \"http://artchives.fondazionezeri.unibo.it/sparql\"\n",
        "\n",
        "results = \"\"\"\n",
        "{\n",
        "  \"head\" : {\n",
        "    \"vars\" : [ \"class\", \"tot\" ]\n",
        "  },\n",
        "  \"results\" : {\n",
        "    \"bindings\" : [ {\n",
        "      \"class\" : {\n",
        "        \"type\" : \"uri\",\n",
        "        \"value\" : \"http://www.wikidata.org/entity/Q31855\"\n",
        "      },\n",
        "      \"tot\" : {\n",
        "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
        "        \"type\" : \"literal\",\n",
        "        \"value\" : \"6\"\n",
        "      }\n",
        "    }, {\n",
        "      \"class\" : {\n",
        "        \"type\" : \"uri\",\n",
        "        \"value\" : \"http://www.wikidata.org/entity/Q5\"\n",
        "      },\n",
        "      \"tot\" : {\n",
        "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
        "        \"type\" : \"literal\",\n",
        "        \"value\" : \"25\"\n",
        "      }\n",
        "    }, {\n",
        "      \"class\" : {\n",
        "        \"type\" : \"uri\",\n",
        "        \"value\" : \"http://www.wikidata.org/entity/Q9388534\"\n",
        "      },\n",
        "      \"tot\" : {\n",
        "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
        "        \"type\" : \"literal\",\n",
        "        \"value\" : \"26\"\n",
        "      }\n",
        "    } ]\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iECVWymaGSzE"
      },
      "source": [
        "The JSON returned by **any SPARQL query** has always the same structure, namely: a dictionary with two keys: `head` and `results`.\n",
        "\n",
        "<pre>\n",
        "{\n",
        "  <span style=\"color:blue\">\"head\"</span> : {\n",
        "    \"vars\" : [ \"class\", \"tot\" ]\n",
        "  },\n",
        "  <span style=\"color:blue\">\"results\"</span> : {\n",
        "    \"bindings\" : [ {\n",
        "      \"class\" : {\n",
        "...\n",
        "</pre>\n",
        "\n",
        "**HEADINGS** \n",
        "\n",
        "The value of the key `head` is a dictionary with only one key called `vars`. The value of `vars` is a list including all the query variables (in the same order as in the query). In our case the query variables are `[\"class\", \"tot\"]`, and these correspond to the names of columns in the tabular view of results\n",
        " \n",
        "<pre>\n",
        "{\n",
        "  <b>\"head\"</b> : {\n",
        "    <span style=\"color:blue\">\"vars\" : [ \"class\", \"tot\" ]</span>\n",
        "  },\n",
        "  \"results\" : {\n",
        "    \"bindings\" : [ {\n",
        "      \"class\" : {\n",
        "...\n",
        "</pre>\n",
        "\n",
        "**RESULTS GROUPING** \n",
        "\n",
        "The value of the key `results` is a dictionary with only one key called `bindings`, whose value is a list of dictionaries. \n",
        "\n",
        "<pre>\n",
        "{\n",
        "  \"head\" : {\n",
        "    \"vars\" : [ \"class\", \"tot\" ]</span>\n",
        "  },\n",
        "  <b>\"results\"</b> : {\n",
        "    <span style=\"color:blue\">\"bindings\"</span> : [ \n",
        "        {...},\n",
        "        {...},\n",
        "        {...}\n",
        "    ]\n",
        "  }\n",
        "}    \n",
        "</pre>\n",
        "\n",
        "**SINGLE RESULT** \n",
        "\n",
        "Each dictionary in the list corresponds to a row of the tabular results.\n",
        "\n",
        "Each dictionary/row includes as many dictionaries as the number of query variables (in our case: `class` and `tot`). The keys in the dictionary/row are the names of the column/query varaiables.\n",
        "\n",
        "<pre>\n",
        "{\n",
        "  \"head\" : {\n",
        "    \"vars\" : [ \"class\", \"tot\" ]</span>\n",
        "  },\n",
        "  <b>\"results\"</b> : {\n",
        "    \"bindings\" : [ \n",
        "        <span style=\"color:blue\">{\n",
        "            \"class\": {...},\n",
        "            \"tot\": {...}\n",
        "        }</span>,\n",
        "        <span style=\"color:blue\">{\n",
        "            \"class\": {...},\n",
        "            \"tot\": {...}\n",
        "        }</span>,\n",
        "        <span style=\"color:blue\">{\n",
        "            \"class\": {...},\n",
        "            \"tot\": {...}\n",
        "        }</span>\n",
        "    ]\n",
        "  }\n",
        "}    \n",
        "</pre>\n",
        "\n",
        "**PARTS OF A SINGLE RESULT** \n",
        "\n",
        "The value of the key/column (class, tot) corresponds to a cell of the tabular result. For every cell two/three variables are recorded according to the type of value, namely:\n",
        "\n",
        " * the type, that can be either `uri` or `literal`\n",
        "\n",
        " <pre>\n",
        "      \"class\" : {\n",
        "        <span style=\"color:blue\">\"type\" : \"uri\",</span>\n",
        "        \"value\" : \"http://www.wikidata.org/entity/Q31855\"\n",
        "      },\n",
        "      \"tot\" : {\n",
        "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
        "        <span style=\"color:blue\">\"type\" : \"literal\",</span>\n",
        "        \"value\" : \"6\"\n",
        "      }\n",
        " </pre>\n",
        "\n",
        " * the actual `value` (either the http URI or the string)\n",
        " \n",
        " <pre>\n",
        "      \"class\" : {\n",
        "        \"type\" : \"uri\",\n",
        "        <span style=\"color:blue\">\"value\" : \"http://www.wikidata.org/entity/Q31855\"</span>\n",
        "      },\n",
        "      \"tot\" : {\n",
        "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
        "        \"type\" : \"literal\",\n",
        "        <span style=\"color:blue\">\"value\" : \"6\"</span>\n",
        "      }\n",
        " </pre>\n",
        " \n",
        " * if literal, the `datatype` of the literal:\n",
        " \n",
        " <pre>\n",
        "      \"class\" : {\n",
        "        \"type\" : \"uri\",\n",
        "        \"value\" : \"http://www.wikidata.org/entity/Q31855\"\n",
        "      },\n",
        "      \"tot\" : {\n",
        "        <span style=\"color:blue\">\"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",</span>\n",
        "        \"type\" : \"literal\",\n",
        "        \"value\" : \"6\"\n",
        "      }\n",
        "</pre>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Parse a SPARQL-JSON result\n",
        "\n",
        "**Download the results** of the query in a file called `sparql_query_result.json` or query the online version of the same file in our repository.\n",
        "\n",
        "To query SPARQL-JSON results we need a couple of python libraries:\n",
        "\n",
        " * use `json` to parse the object into a dictionary\n",
        " * if the data to be queried are online, use `requests` to access the remote file\n"
      ],
      "metadata": {
        "id": "Kz21_TF6hOwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUWuKBDqGSzE",
        "outputId": "3b686e56-b8ce-429e-ebb4-fff33497a55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'head': {'vars': ['class', 'tot']},\n",
            " 'results': {'bindings': [{'class': {'type': 'uri',\n",
            "                                     'value': 'http://www.wikidata.org/entity/Q31855'},\n",
            "                           'tot': {'datatype': 'http://www.w3.org/2001/XMLSchema#integer',\n",
            "                                   'type': 'literal',\n",
            "                                   'value': '6'}},\n",
            "                          {'class': {'type': 'uri',\n",
            "                                     'value': 'http://www.wikidata.org/entity/Q5'},\n",
            "                           'tot': {'datatype': 'http://www.w3.org/2001/XMLSchema#integer',\n",
            "                                   'type': 'literal',\n",
            "                                   'value': '25'}},\n",
            "                          {'class': {'type': 'uri',\n",
            "                                     'value': 'http://www.wikidata.org/entity/Q9388534'},\n",
            "                           'tot': {'datatype': 'http://www.w3.org/2001/XMLSchema#integer',\n",
            "                                   'type': 'literal',\n",
            "                                   'value': '26'}}]}}\n"
          ]
        }
      ],
      "source": [
        "import json , pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=1) # just to pretty print results\n",
        "\n",
        "# uncomment if you run the code locally\n",
        "# with open('../resources/sparql_query_result.json','r') as results:\n",
        "#    data = json.load(results)  \n",
        "#    pprint.pprint(data)\n",
        "\n",
        "# if you run the code in colab\n",
        "import requests\n",
        "response = requests.get(\"https://raw.githubusercontent.com/marilenadaquino/information_visualization/main/2022-2023/resources/sparql_query_result.json\")\n",
        "data = json.loads(response.text)\n",
        "pprint.pprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n7HOP-vGSzF"
      },
      "source": [
        "Get colum names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J3m-IiKGSzF",
        "outputId": "d6b1e95c-f687-4ff2-ca1c-b9f60b72e388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\n",
            "tot\n"
          ]
        }
      ],
      "source": [
        "for column in data[\"head\"][\"vars\"]: # enter the list\n",
        "    print(column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT8NluEiGSzF"
      },
      "source": [
        "Get results and print the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fETOpovGSzF",
        "outputId": "f0afdb9b-a0c6-4443-cc9f-730218097ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The class http://www.wikidata.org/entity/Q31855 has 6 individuals\n",
            "The class http://www.wikidata.org/entity/Q5 has 25 individuals\n",
            "The class http://www.wikidata.org/entity/Q9388534 has 26 individuals\n"
          ]
        }
      ],
      "source": [
        "for result in data[\"results\"][\"bindings\"]:  # enter the list of dictionaries // do you remember \"for row in rows\"?\n",
        "    res_class = result['class']['value']    # the value of the cell under column \"class\"\n",
        "    res_tot = result['tot']['value']        # the value of the cell under column \"tot\"\n",
        "    print('The class', res_class,'has', res_tot, 'individuals')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULZ8BPlzGSzG"
      },
      "source": [
        "### 2.3 Query a SPARQL endpoint with RDFLib and SPARQLWrapper libraries\n",
        "\n",
        "Querying data via SPARQL endpoint GUI is not always convenient: \n",
        "\n",
        " * it is highly discouraged when you need to show your work as a one-click application. \n",
        " * in many cases, downloading data locally and parse them in a RDFLib graph it's not possible or convenient, e.g. dumping the entire Wikidata graph would require you ~60GB storage (only for the zipped file!). \n",
        " * while the online data keep being updated, the local copy goes easily **out-to-date**.\n",
        "\n",
        "[SPARQLWrapper](https://sparqlwrapper.readthedocs.io/en/latest/main.html) is an extension of RDFLib that allows you to **query remote SPARQL endpoints** and to get up-to-date result data in the same JSON format you'd get if you download result data from the GUI.\n",
        "\n",
        "In order to query a remote SPARQL endpoint you'll need:\n",
        "\n",
        " * If you have mac *you may need* to tweak the certificates (use an unverified one) for querying an external service \n",
        " * get the URL of the API of the SPARQL endpoint (sometimes it is the same as the URL of the GUI)\n",
        " * prepare the SPARQL query \n",
        " * create the wrapper around the SPARQL API (via SPARQLWrapper library)\n",
        " * send the query and get the JSON results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktUGOaPpGSzG",
        "outputId": "22f0e5f1-03de-405d-d28e-9340f27192fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://wikiba.se/ontology#Dump http://creativecommons.org/ns#license http://creativecommons.org/publicdomain/zero/1.0/\n",
            "http://wikiba.se/ontology#Dump http://schema.org/softwareVersion 1.0.0\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:01Z\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:04Z\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:08Z\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:12Z\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:17Z\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:21Z\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:26Z\n",
            "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2021-09-24T23:00:29Z\n"
          ]
        }
      ],
      "source": [
        "# uncomment if you run on colab\n",
        "#!pip install SPARQLWrapper\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import ssl\n",
        "\n",
        "# if you have mac and you run this locally, uncomment\n",
        "#ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# get the endpoint API\n",
        "wikidata_endpoint = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n",
        "\n",
        "# prepare the query : 10 random triples\n",
        "my_SPARQL_query = \"\"\"\n",
        "SELECT *\n",
        "WHERE {?s ?p ?o}\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "# set the endpoint \n",
        "sparql_wd = SPARQLWrapper(wikidata_endpoint)\n",
        "# set the query\n",
        "sparql_wd.setQuery(my_SPARQL_query)\n",
        "# set the returned format\n",
        "sparql_wd.setReturnFormat(JSON)\n",
        "# get the results\n",
        "results = sparql_wd.query().convert()\n",
        "\n",
        "# manipulate the result\n",
        "for result in results[\"results\"][\"bindings\"]:\n",
        "    print(result[\"s\"][\"value\"], result[\"p\"][\"value\"], result[\"o\"][\"value\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOZAzhx0GSzG"
      },
      "source": [
        "## 3. Data integration \n",
        "\n",
        "### An example: integrate art historians' birth places from Wikidata\n",
        "\n",
        "In the last tutorial we saw how to add triples about historians' birthplaces (`wdt:P19`). We looked in wikidata for the URI of the birth place of Federico Zeri and we manually added the new triple  to the graph. \n",
        "\n",
        "Now we can do this operation **systematically**:\n",
        "*For every art historian in ARTchives that is also in Wikidata, we get their birthplaces and we add this information to our graph*\n",
        "\n",
        "To do that we need to:\n",
        "\n",
        " 1. get the list of historians in ARTchives that are also available in Wikidata\n",
        " 2. prepare a SPARQL query that returns the birthplace of a list of art historians \n",
        " 3. we send the query to Wikidata\n",
        " 4. if there is a result, we add a triple to our graph\n",
        " \n",
        "\n",
        "### 3.1 Get the list of historians\n",
        "\n",
        "We need to take into account a couple of caveats.\n",
        " \n",
        " * How to distinguish historians from other people that are mentioned in ARTchives? We use the pattern `?collection wdt.P170(has creator) ?creator`, which is the only mandatory predicate that distinguishes historians from other people.\n",
        " * How do we select only the historians that are both in ARTchives and Wikidata? We match a substring in the URI (if it includes the substring \"wikidata.org/entity/\", we are sure this is a Wikidata entity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FgUW78fGSzG",
        "outputId": "cc4643e5-2013-41f1-e28f-3f47183a3d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<http://www.wikidata.org/entity/Q537874>', '<http://www.wikidata.org/entity/Q19997512>', '<http://www.wikidata.org/entity/Q55453618>', '<http://www.wikidata.org/entity/Q1373290>', '<http://www.wikidata.org/entity/Q1629748>', '<http://www.wikidata.org/entity/Q41616785>', '<http://www.wikidata.org/entity/Q1641821>', '<http://www.wikidata.org/entity/Q1271052>', '<http://www.wikidata.org/entity/Q60185>', '<http://www.wikidata.org/entity/Q1296486>', '<http://www.wikidata.org/entity/Q457739>', '<http://www.wikidata.org/entity/Q2824734>', '<http://www.wikidata.org/entity/Q88907>', '<http://www.wikidata.org/entity/Q1715096>', '<http://www.wikidata.org/entity/Q18935222>', '<http://www.wikidata.org/entity/Q995470>', '<http://www.wikidata.org/entity/Q1712683>', '<http://www.wikidata.org/entity/Q3051533>', '<http://www.wikidata.org/entity/Q1089074>', '<http://www.wikidata.org/entity/Q3057287>', '<http://www.wikidata.org/entity/Q90407>', '<http://www.wikidata.org/entity/Q85761254>', '<http://www.wikidata.org/entity/Q61913691>', '<http://www.wikidata.org/entity/Q6700132>'}\n"
          ]
        }
      ],
      "source": [
        "from rdflib import Namespace , Literal , URIRef\n",
        "from rdflib.namespace import RDF , RDFS\n",
        "\n",
        "# bind the uncommon namespaces\n",
        "wd = Namespace(\"http://www.wikidata.org/entity/\") # remember that a prefix matches a URI until the last slash (or hashtag #)\n",
        "wdt = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
        "art = Namespace(\"https://w3id.org/artchives/\")\n",
        "\n",
        "# Get the list of art historians in our graph \"g\"\n",
        "arthistorians_list = set()\n",
        "\n",
        "# iterate over the triples in the graph\n",
        "for s,p,o in g.triples(( None, wdt.P170, None)):   # people \"o\" are the creator \"wdt.P170\" of a collection \"s\"\n",
        "    if \"wikidata.org/entity/\" in str(o):           # look for the substring to filter wikidata entities only\n",
        "        arthistorians_list.add('<' + str(o) + '>')     # remember to transform them in strings! \n",
        "    \n",
        "print(arthistorians_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the code above:\n",
        "\n",
        "* We iterate over the triples that respect the pattern to get the URIs of the historians. \n",
        " * Then we parse URIs (that RDFLIb considers as `RDFLib.URIRef`) as strings. \n",
        " * we match the substing of Wikidata URIs in our strings\n",
        " * We create a list of URIs to be used in a SPARQL query with the `VALUES` operator, therefore we wrap the URI strings in the hooks `<>` \n",
        " *  we add these URIs to a set (a list of unique values)"
      ],
      "metadata": {
        "id": "131lsEn8LB82"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSCOY4VJGSzH"
      },
      "source": [
        "\n",
        "### 3.2 Prepare the query\n",
        "\n",
        "We have two options:\n",
        " * for each URI, we prepare a query to be sent to Wikidata. However, this implies sending many small queries to an external service that may have (reasonably) imposed a query limit (If you ever get an error `429: Too Many requests`, see [here](https://stackoverflow.com/questions/62396801/how-to-handle-too-many-requests-on-wikidata-using-sparqlwrapper) the reason.)\n",
        " * **BETTER** we send only one (bigger) query to the Wikidata endpoint where all the historians are terms specified in a `VALUES` list. The result table of our query will include for every row (1) the URI of the historian, (2) the URI of the birthplace, and (3) the label of the birth place *in english only!* (be aware that Wikidata has labels for every language!)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the values to be queried\n",
        "historians = ' '.join(arthistorians_list) # <uri1> <uri2> <uri3> ... <uriN>\n",
        "\n",
        "# prepare the query\n",
        "birthplace_query = \"\"\"\n",
        "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
        "SELECT DISTINCT ?historian ?birthplace ?birthplace_label\n",
        "WHERE {\n",
        "    VALUES ?historian {\"\"\"+historians+\"\"\"} . # look how we include a variable in a query string!\n",
        "    ?historian wdt:P19 ?birthplace . \n",
        "    ?birthplace rdfs:label ?birthplace_label .\n",
        "    FILTER (langMatches(lang(?birthplace_label), \"EN\"))\n",
        "    } \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PRKmaVMtLcry"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "### 3.3 Send the query to Wikidata\n"
      ],
      "metadata": {
        "id": "MMksHDllLfRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cG_pzzkjGSzH"
      },
      "outputs": [],
      "source": [
        "# set the endpoint \n",
        "sparql_wd = SPARQLWrapper(wikidata_endpoint)\n",
        "# set the query\n",
        "sparql_wd.setQuery(birthplace_query)\n",
        "# set the returned format\n",
        "sparql_wd.setReturnFormat(JSON)\n",
        "# get the results\n",
        "results = sparql_wd.query().convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-jt-o6DGSzH"
      },
      "source": [
        "**INTEGRATE THE DATA INTO THE GRAPH** Once the wrapper and the query are set we manipulate results:\n",
        " \n",
        " * only if the birthplace is found we look also for its label \n",
        " * only for those birthplaces that have both URI and label we create a new triple to be added to our graph.\n",
        " \n",
        "**STORE** Now that we have added all these new triples to our in-memory graph, we can store these data into a new file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnnQrsaiGSzH"
      },
      "outputs": [],
      "source": [
        "# manipulate the result\n",
        "for result in results[\"results\"][\"bindings\"]:\n",
        "    historian_uri = result[\"historian\"][\"value\"]\n",
        "    print(\"historian:\", historian_uri)\n",
        "    if \"birthplace\" in result: # some historians may have no birthplace recorded in Wikidata!\n",
        "        birthplace = result[\"birthplace\"][\"value\"]\n",
        "        if \"birthplace_label\" in result: \n",
        "            birthplace_label = result[\"birthplace_label\"][\"value\"]\n",
        "            print(\"found:\", birthplace, birthplace_label)\n",
        "            \n",
        "            # only if both uri and label are found we add them to the graph\n",
        "            g.add(( URIRef(historian_uri) , URIRef(wdt.P19) , URIRef(birthplace) ))\n",
        "            g.add(( URIRef(birthplace) , RDFS.label , Literal(birthplace_label) ))\n",
        "    else:\n",
        "        print(\"nothing found in wikidata :(\")\n",
        "\n",
        "g.serialize(destination='artchives_birthplaces.nq', format='nquads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUwM1WUyGSzI"
      },
      "source": [
        "## For your project\n",
        "\n",
        "If you want to use ARTchives in your project, you can:\n",
        " \n",
        " * query and manipulate ARTchives locally (use the `artchives.nq` file) via RDFlib methods or local SPARQL queries\n",
        " * query external sources remotely (Wikidata and others) using SPARQLWrapper\n",
        " * save the data extracted from external sources along with ARTchives data locally (create a new file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRuTEUa4GSzI"
      },
      "source": [
        "## Resources \n",
        "\n",
        "Some SPARQL tutorials:\n",
        "  \n",
        " * [SPARQL Tutorial - Apache](https://jena.apache.org/tutorials/sparql.html)\n",
        " * [SPARQL tutorial - stardog](https://www.stardog.com/tutorials/sparql/)\n",
        " * [SPARQL tutorial - Programming historian](https://programminghistorian.org/en/lessons/retired/graph-databases-and-SPARQL)\n",
        " * [SPARQL tutorial - Wikidata](https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial) - very useful for your project! It teaches you how to query data on Wikidata \n",
        " \n",
        "SPARQL and RDFLib:\n",
        "   \n",
        " * [RDFLib documentation on SPARQL](https://rdflib.readthedocs.io/en/stable/intro_to_sparql.html)\n",
        " \n",
        "SPARQLWrapper:\n",
        " \n",
        " * [SPARQLWrapper documentation](https://sparqlwrapper.readthedocs.io/en/latest/main.html)\n",
        " \n",
        "Wikidata resources:\n",
        " * [index of categories](https://www.wikidata.org/wiki/Category:Wikidata:SPARQL_query_service)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "sparql_tutorial.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}